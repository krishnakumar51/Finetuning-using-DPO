{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOisShAny1xxD1kbqFJGWaz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/krishnakumar51/Finetuning-using-DPO/blob/main/Finetuning_Using_DPO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SvID1p_i9M2m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4427d87b-0375-4b6c-fe05-c686b251ccc7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: transformers\n",
            "Version: 4.41.2\n",
            "Summary: State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow\n",
            "Home-page: https://github.com/huggingface/transformers\n",
            "Author: The Hugging Face team (past and future) with the help of all our contributors (https://github.com/huggingface/transformers/graphs/contributors)\n",
            "Author-email: transformers@huggingface.co\n",
            "License: Apache 2.0 License\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: filelock, huggingface-hub, numpy, packaging, pyyaml, regex, requests, safetensors, tokenizers, tqdm\n",
            "Required-by: \n"
          ]
        }
      ],
      "source": [
        "!pip show transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install trl peft accelerate datasets bitsandbytes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kmcPfpMuaLpz",
        "outputId": "80077bc2-3fb2-4c78-81db-1d2b16d7815c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting trl\n",
            "  Downloading trl-0.9.4-py3-none-any.whl (226 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.7/226.7 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting peft\n",
            "  Downloading peft-0.11.1-py3-none-any.whl (251 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.6/251.6 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting accelerate\n",
            "  Downloading accelerate-0.30.1-py3-none-any.whl (302 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.6/302.6 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting datasets\n",
            "  Downloading datasets-2.19.2-py3-none-any.whl (542 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.1/542.1 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl (119.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.8/119.8 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from trl) (2.3.0+cu121)\n",
            "Requirement already satisfied: transformers>=4.31.0 in /usr/local/lib/python3.10/dist-packages (from trl) (4.41.2)\n",
            "Requirement already satisfied: numpy>=1.18.2 in /usr/local/lib/python3.10/dist-packages (from trl) (1.25.2)\n",
            "Collecting tyro>=0.5.11 (from trl)\n",
            "  Downloading tyro-0.8.4-py3-none-any.whl (102 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.4/102.4 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from peft) (24.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from peft) (6.0.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from peft) (4.66.4)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from peft) (0.4.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.17.0 in /usr/local/lib/python3.10/dist-packages (from peft) (0.23.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.14.0)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Collecting requests>=2.32.1 (from datasets)\n",
            "  Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (4.12.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.1->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.1->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.1->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.1->datasets) (2024.6.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (3.1.4)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.4.0->trl)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.4.0->trl)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.4.0->trl)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.4.0->trl)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.4.0->trl)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.4.0->trl)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.4.0->trl)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.4.0->trl)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.4.0->trl)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.4.0->trl)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.4.0->trl)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.4.0->trl)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m48.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->trl) (2024.5.15)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->trl) (0.19.1)\n",
            "Requirement already satisfied: docstring-parser>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl) (0.16)\n",
            "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl) (13.7.1)\n",
            "Collecting shtab>=1.5.6 (from tyro>=0.5.11->trl)\n",
            "  Downloading shtab-1.7.1-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (2.16.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.4.0->trl) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.4.0->trl) (1.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl) (0.1.2)\n",
            "Installing collected packages: xxhash, shtab, requests, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, dill, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, tyro, nvidia-cusolver-cu12, datasets, bitsandbytes, accelerate, trl, peft\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.31.0\n",
            "    Uninstalling requests-2.31.0:\n",
            "      Successfully uninstalled requests-2.31.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed accelerate-0.30.1 bitsandbytes-0.43.1 datasets-2.19.2 dill-0.3.8 multiprocess-0.70.16 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 peft-0.11.1 requests-2.32.3 shtab-1.7.1 trl-0.9.4 tyro-0.8.4 xxhash-3.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145,
          "referenced_widgets": [
            "d8764b41e63342c4b864de907ca37c3f",
            "1278b4d153554c35bb5ae99c03e3b56a",
            "12fbaefc72424d7d940f4304cfcb389a",
            "8da09439d11a4f168c205d4bbdedca5e",
            "d665129a3e0145d3a90ebbed138b4797",
            "71d9417504de4a0d862e418caf8abdf0",
            "9f74e5f65e7c4077bfe188750a9c9bd8",
            "5a86169f30ae447d8f40492882673d85",
            "84ca790167724d08abfb5dc3abc17b9d",
            "23c3897d8b494ffeb5dfb93f08c1795c",
            "62087d8cea3b448695be75481ad0abe8",
            "245899e24e244012978e22fcd4cc8700",
            "8879077a5e6747abb48c6978a7d28ac0",
            "10d579dcb32b4a849baa5be6b8273db2",
            "6f81b87105e345d280c505b6dd4f6bd9",
            "63d361c5869140208e01945235e623b5",
            "781a2506e7d94e88a16b916243c60987",
            "dcbed067b97a48089a87022e2e3e254b",
            "138be09d1dac4b2c87177a9409d65093",
            "936642b93d8f47b7b4f85eba7489454f",
            "a2988ce669014fc6ad9f26964fac51f2",
            "0197b5e47b4744878dcc5716ca5c1b5a",
            "654ade69691c48a4898ad83692ff1451",
            "7b09d0eeb8914946b23f578b702bb1bc",
            "d56520661ed1435f9612e47fb5a713f6",
            "3699bc5155a743af9b29507e14a94038",
            "c8c68ce298854287b971b8a64b182cb2",
            "7873db7eae624ffdb487476cd728a163",
            "d9fb30e2327e49efb679a03ee5043a9f",
            "86ac3c88c759459c980bdfbac2f34260",
            "8cdfbc6735534b4eaae7ef4ac482180b",
            "7c3926bb00b246ec8abd339f4684e2ed"
          ]
        },
        "id": "hz8N_oGZbig7",
        "outputId": "fae777d6-65b9-4b98-f2a9-166469a05f2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d8764b41e63342c4b864de907ca37c3f"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM"
      ],
      "metadata": {
        "id": "c9BBtQ3kTLLJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from datasets import load_dataset,Dataset"
      ],
      "metadata": {
        "id": "cTd3PH1BTnY2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTTrainer"
      ],
      "metadata": {
        "id": "C-pgdkOLULZG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from accelerate import Accelerator"
      ],
      "metadata": {
        "id": "6sk0aOsMUOsN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "metadata": {
        "id": "dUTuX_RyUTB2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, AutoPeftModelForCausalLM, prepare_model_for_kbit_training, get_peft_model"
      ],
      "metadata": {
        "id": "ZCyAYoFcUUKF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments"
      ],
      "metadata": {
        "id": "KBNuqX0xUXhR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sft_config={\n",
        "\n",
        "            \"model_ckpt\": \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
        "            \"load_in_4bit\": True,\n",
        "            \"device_map\": {\"\": Accelerator().local_process_index},\n",
        "            \"torch_dtype\": torch.float16,\n",
        "            \"torch_dtype\": torch.float16,\n",
        "            \"trust_remote_code\": True,\n",
        "\n",
        "            \"use_lora\": True,\n",
        "            \"r\": 16,\n",
        "            \"lora_alpha\": 16,\n",
        "            \"lora_dropout\": 0.05,\n",
        "            \"bias\": \"none\",\n",
        "            \"task_type\": \"CAUSAL_LM\",\n",
        "            \"target_modules\": [\"q_proj\", \"v_proj\"],\n",
        "\n",
        "            \"output_dir\": \"sft-tiny-chatbot\",\n",
        "            \"per_device_train_batch_size\": 1,\n",
        "            \"gradient_accumulation_steps\": 1,\n",
        "            \"optim\": \"paged_adamw_32bit\",\n",
        "            \"learning_rate\": 2e-4,\n",
        "            \"lr_scheduler_type\": \"cosine\",\n",
        "            \"save_strategy\": \"epoch\",\n",
        "            \"logging_steps\": 100,\n",
        "            \"num_train_epochs\": 1,\n",
        "            \"max_steps\": 250,\n",
        "            \"fp16\": True,\n",
        "            \"push_to_hub\": False,\n",
        "            \"train_cln_name\": \"text\",\n",
        "            \"packing\": False,\n",
        "            \"max_seq_length\": 512,\n",
        "            \"neftune_noise_alpha\": 5\n",
        "\n",
        "}"
      ],
      "metadata": {
        "id": "i_yZdf1RUc-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TrainSFT:\n",
        "\n",
        "  def __init__(self,data,config):\n",
        "    self.data=data\n",
        "    self.config=config\n",
        "\n",
        "  def prepare_lora_model(self):\n",
        "\n",
        "    self.lora_config= LoraConfig(r=self.config[\"r\"],\n",
        "                                    lora_alpha=self.config[\"lora_alpha\"],\n",
        "                                    lora_dropout=self.config[\"lora_dropout\"],\n",
        "                                    bias=self.config[\"bias\"],\n",
        "                                    task_type=self.config[\"task_type\"],\n",
        "                                    target_modules=self.config[\"target_modules\"])\n",
        "\n",
        "    self.model= get_peft_model(self.model,self.lora_config)\n",
        "\n",
        "  def load_model_tokenizer(self):\n",
        "    self.model = AutoModelForCausalLM.from_pretrained(\n",
        "                            self.config[\"model_ckpt\"],\n",
        "                            load_in_4bit=self.config[\"load_in_4bit\"],\n",
        "                            device_map=self.config[\"device_map\"],\n",
        "                            torch_dtype=self.config[\"torch_dtype\"])\n",
        "    self.model.config.use_cache=False\n",
        "    self.model.config.pretraining_tp=1\n",
        "    self.model = prepare_model_for_kbit_training(self.model)\n",
        "\n",
        "    if self.config[\"use_lora\"]:\n",
        "      self.prepare_lora_model()\n",
        "\n",
        "    self.tokenizer = AutoTokenizer.from_pretrained(self.config[\"model_ckpt\"])\n",
        "    self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "\n",
        "  def set_training_args(self):\n",
        "\n",
        "\n",
        "    return TrainingArguments(\n",
        "                                    output_dir=self.config[\"output_dir\"],\n",
        "                                    per_device_train_batch_size=self.config[\"per_device_train_batch_size\"],\n",
        "                                    gradient_accumulation_steps=self.config[\"gradient_accumulation_steps\"],\n",
        "                                    optim=self.config[\"optim\"],\n",
        "                                    learning_rate=self.config[\"learning_rate\"],\n",
        "                                    lr_scheduler_type=self.config[\"lr_scheduler_type\"],\n",
        "                                    save_strategy=self.config[\"save_strategy\"],\n",
        "                                    logging_steps=self.config[\"logging_steps\"],\n",
        "                                    num_train_epochs=self.config[\"num_train_epochs\"],\n",
        "                                    max_steps=self.config[\"max_steps\"],\n",
        "                                    fp16=self.config[\"fp16\"],\n",
        "                                    push_to_hub=self.config[\"push_to_hub\"],\n",
        "                                    neftune_noise_alpha=self.config[\"neftune_noise_alpha\"]\n",
        "                                )\n",
        "  def create_trainer(self):\n",
        "\n",
        "        self.load_model_tokenizer()\n",
        "        if self.config[\"use_lora\"]:\n",
        "            print(self.model.print_trainable_parameters())\n",
        "            self.trainer = SFTTrainer(\n",
        "                                    model=self.model,\n",
        "                                    train_dataset=self.data,\n",
        "                                    peft_config=self.lora_config,\n",
        "                                    dataset_text_field=self.config[\"train_cln_name\"],\n",
        "                                    args=self.set_training_args(),\n",
        "                                    tokenizer=self.tokenizer,\n",
        "                                    packing=self.config[\"packing\"],\n",
        "                                    max_seq_length=self.config[\"max_seq_length\"]\n",
        "                                )\n",
        "        else:\n",
        "            self.trainer = SFTTrainer(\n",
        "                                    model=self.model,\n",
        "                                    train_dataset=self.data,\n",
        "                                    dataset_text_field=self.config[\"train_cln_name\"],\n",
        "                                    args=self.set_training_args(),\n",
        "                                    tokenizer=self.tokenizer,\n",
        "                                    packing=self.config[\"packing\"],\n",
        "                                    max_seq_length=self.config[\"max_seq_length\"]\n",
        "                                )\n",
        "\n",
        "  def train_and_save_model(self):\n",
        "    self.create_trainer()\n",
        "    self.trainer.train()\n",
        "    self.trainer.save_model(self.config[\"output_dir\"])\n",
        "    self.tokenizer.save_pretrained(self.config[\"output_dir\"])\n",
        "\n"
      ],
      "metadata": {
        "id": "v7XEvuZpVzfU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from datasets import load_dataset,Dataset"
      ],
      "metadata": {
        "id": "Ivle21kXf0xY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_data():\n",
        "    data = load_dataset(\"tatsu-lab/alpaca\", split=\"train\")\n",
        "    data_df = data.to_pandas()\n",
        "    data_df = data_df[:700]\n",
        "    data_df[\"text\"] = data_df[[\"input\", \"instruction\", \"output\"]].apply(lambda x: \"Human: \" + x[\"instruction\"] + \" \" + x[\"input\"] + \" Assistant: \"+ x[\"output\"], axis=1)\n",
        "    data = Dataset.from_pandas(data_df)\n",
        "    return data\n",
        "\n",
        "data = create_data()"
      ],
      "metadata": {
        "id": "BYs4OEGSYu9p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_sft = TrainSFT(data, sft_config)\n",
        "train_sft.train_and_save_model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "6404d8ef31b345478d2a063164286e2d",
            "07e415e95ae34e85ab7f6bbd9619fb9f",
            "08d748edb4be495782004f03895244a4",
            "92e3f3e5b47f4532b220d401805f088b",
            "2d45b48cb9ae4e908f7302d8b7163413",
            "c5d4efcce9f54d1fad8c11534cfe86e0",
            "c421c9195d1b495b959d93e9bea872f6",
            "a7c1cb8e2dd74d8ab2b399db561689cd",
            "0b72ec2a212c424cb3abf0876652df6e",
            "15346975a8fc40ce85b125bcb4cb33b6",
            "a82ff817ab60450fb646a6b1c6d43854"
          ]
        },
        "id": "zwLnDH4kY9zG",
        "outputId": "93253d3e-5e79-4180-b86f-4070058b0a03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 2,252,800 || all params: 1,102,301,184 || trainable%: 0.2044\n",
            "None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, max_seq_length. Will not be supported from version '1.0.0'.\n",
            "\n",
            "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
            "  warnings.warn(message, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1965: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:307: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/700 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6404d8ef31b345478d2a063164286e2d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "max_steps is given, it will override any value given in num_train_epochs\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/nn/modules.py:426: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [250/250 01:31, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>2.164800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>1.797800</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preference Alignmemnt- DPO"
      ],
      "metadata": {
        "id": "PmBLrkCZf-66"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dpo_config = {\n",
        "            \"model_ckpt\": \"snshrivas10/sft-tiny-chatbot\",\n",
        "            \"load_in_4bit\": True,\n",
        "            \"device_map\": {\"\": Accelerator().local_process_index},\n",
        "            \"torch_dtype\": torch.float16,\n",
        "            \"trust_remote_code\": True,\n",
        "            \"use_lora\": True,\n",
        "            \"r\": 8,\n",
        "            \"lora_alpha\": 8,\n",
        "            \"lora_dropout\": 0.05,\n",
        "            \"bias\": \"none\",\n",
        "            \"task_type\": \"CAUSAL_LM\",\n",
        "            \"target_modules\": [\"q_proj\", \"v_proj\"],\n",
        "            \"output_dir\": \"tiny-chatbot-dpo\",\n",
        "            \"per_device_train_batch_size\": 1,\n",
        "            \"gradient_accumulation_steps\": 1,\n",
        "            \"optim\": \"paged_adamw_32bit\",\n",
        "            \"learning_rate\": 2e-4,\n",
        "            \"lr_scheduler_type\": \"cosine\",\n",
        "            \"save_strategy\": \"epoch\",\n",
        "            \"logging_steps\": 100,\n",
        "            \"num_train_epochs\": 1,\n",
        "            \"max_steps\": 250,\n",
        "            \"fp16\": True,\n",
        "            \"push_to_hub\": True,\n",
        "            \"train_cln_name\": \"text\",\n",
        "            \"packing\": False,\n",
        "            \"neftune_noise_alpha\": 5,\n",
        "            \"beta\": 0.1,\n",
        "            \"loss_type\": \"kto_pair\",\n",
        "            \"max_length\": 768,\n",
        "            \"max_prompt_length\": 512,\n",
        "            \"max_target_length\": 256,\n",
        "            \"is_encoder_decoder\": False\n",
        "        }"
      ],
      "metadata": {
        "id": "J-s8eLn3hZ0Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from datasets import load_dataset, Dataset\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments\n",
        "from trl import DPOTrainer\n",
        "import pandas as pd\n",
        "from accelerate import Accelerator\n",
        "from peft import get_peft_model, LoraConfig"
      ],
      "metadata": {
        "id": "qvM003sKgDGM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TrainDPO:\n",
        "\n",
        "  def __init__(self,data,config):\n",
        "    self.data=data\n",
        "    self.config=config\n",
        "\n",
        "  def prepare_lora_model(self):\n",
        "\n",
        "        self.lora_config = LoraConfig(\n",
        "                                    r=self.config[\"r\"],\n",
        "                                    lora_alpha=self.config[\"lora_alpha\"],\n",
        "                                    lora_dropout=self.config[\"lora_dropout\"],\n",
        "                                    bias=self.config[\"bias\"],\n",
        "                                    task_type=self.config[\"task_type\"],\n",
        "                                    target_modules=self.config[\"target_modules\"]\n",
        "                                )\n",
        "        self.model = get_peft_model(self.model, self.lora_config)\n",
        "        self.model_ref = get_peft_model(self.model_ref, self.lora_config)\n",
        "\n",
        "  def load_model_tokenizer(self):\n",
        "\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "                            self.config[\"model_ckpt\"],\n",
        "                            load_in_4bit=self.config[\"load_in_4bit\"],\n",
        "                            device_map=self.config[\"device_map\"],\n",
        "                            torch_dtype=self.config[\"torch_dtype\"]\n",
        "                        )\n",
        "\n",
        "        self.model_ref = AutoModelForCausalLM.from_pretrained(\n",
        "                            self.config[\"model_ckpt\"],\n",
        "                            load_in_4bit=self.config[\"load_in_4bit\"],\n",
        "                            device_map=self.config[\"device_map\"],\n",
        "                            torch_dtype=self.config[\"torch_dtype\"]\n",
        "                        )\n",
        "        self.model.config.use_cache=False\n",
        "        self.model.config.pretraining_tp=1\n",
        "        self.model = prepare_model_for_kbit_training(self.model)\n",
        "        if self.config[\"use_lora\"]:\n",
        "            self.prepare_lora_model()\n",
        "\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(self.config[\"model_ckpt\"])\n",
        "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "\n",
        "  def set_training_args(self):\n",
        "\n",
        "        return TrainingArguments(\n",
        "                                    output_dir=self.config[\"output_dir\"],\n",
        "                                    per_device_train_batch_size=self.config[\"per_device_train_batch_size\"],\n",
        "                                    gradient_accumulation_steps=self.config[\"gradient_accumulation_steps\"],\n",
        "                                    optim=self.config[\"optim\"],\n",
        "                                    learning_rate=self.config[\"learning_rate\"],\n",
        "                                    lr_scheduler_type=self.config[\"lr_scheduler_type\"],\n",
        "                                    save_strategy=self.config[\"save_strategy\"],\n",
        "                                    logging_steps=self.config[\"logging_steps\"],\n",
        "                                    num_train_epochs=self.config[\"num_train_epochs\"],\n",
        "                                    max_steps=self.config[\"max_steps\"],\n",
        "                                    fp16=self.config[\"fp16\"],\n",
        "                                    push_to_hub=self.config[\"push_to_hub\"],\n",
        "                                    neftune_noise_alpha=self.config[\"neftune_noise_alpha\"]\n",
        "                                )\n",
        "\n",
        "\n",
        "  def create_trainer(self):\n",
        "\n",
        "        self.load_model_tokenizer()\n",
        "        if self.config[\"use_lora\"]:\n",
        "            print(self.model.print_trainable_parameters())\n",
        "            self.trainer = DPOTrainer(\n",
        "                                        self.model,\n",
        "                                        self.model_ref,\n",
        "                                        args=self.set_training_args(),\n",
        "                                        train_dataset=self.data,\n",
        "                                        tokenizer=self.tokenizer,\n",
        "                                        beta=self.config[\"beta\"],\n",
        "                                        loss_type=self.config[\"loss_type\"],\n",
        "                                        max_length=self.config[\"max_length\"],\n",
        "                                        max_prompt_length=self.config[\"max_prompt_length\"],\n",
        "                                        max_target_length=self.config[\"max_target_length\"],\n",
        "                                        is_encoder_decoder=self.config[\"is_encoder_decoder\"]\n",
        "                                    )\n",
        "        else:\n",
        "            self.trainer = DPOTrainer(\n",
        "                                        self.model,\n",
        "                                        self.model_ref,\n",
        "                                        peft_config=self.lora_config,\n",
        "                                        args=self.set_training_args(),\n",
        "                                        train_dataset=self.data,\n",
        "                                        tokenizer=self.tokenizer,\n",
        "                                        beta=self.config[\"beta\"],\n",
        "                                        loss_type=self.config[\"loss_type\"],\n",
        "                                        max_length=self.config[\"max_length\"],\n",
        "                                        max_prompt_length=self.config[\"max_prompt_length\"],\n",
        "                                        max_target_length=self.config[\"max_target_length\"],\n",
        "                                        is_encoder_decoder=self.config[\"is_encoder_decoder\"]\n",
        "                                    )\n",
        "  def train_and_save_model(self):\n",
        "    self.create_trainer()\n",
        "    self.trainer.train()\n",
        "    self.trainer.save_model(self.config[\"output_dir\"])\n",
        "    self.tokenizer.save_pretrained(self.config[\"output_dir\"])\n"
      ],
      "metadata": {
        "id": "_Tv9Zogagpf2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_data():\n",
        "    df = load_dataset(\"Anthropic/hh-rlhf\", split=\"train\").to_pandas()\n",
        "    df[\"prompt\"] = df[\"chosen\"].apply(lambda x: x.split(\"Assistant: \")[0])\n",
        "    df[\"chosen\"] = df[\"chosen\"].apply(lambda x: \"Assistant: \"+ x.split(\"Assistant: \")[-1])\n",
        "    df[\"rejected\"] = df[\"rejected\"].apply(lambda x: \"Assistant: \" + x.split(\"Assistant: \")[-1])\n",
        "    df = df.sample(1000)\n",
        "    data = Dataset.from_pandas(df)\n",
        "    return data\n",
        "data = create_data()"
      ],
      "metadata": {
        "id": "wnPzi2dOiWFB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dpo = TrainDPO(data, dpo_config)\n",
        "train_dpo.train_and_save_model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "9709f15dbef945b79303be7b615e62f7",
            "e1c8bb896eaf4f3b8ea2066ad47570ea",
            "eab5e1d9921c41dca026bc075fe457a7",
            "25f3e3356a434467ab2f2f4b7a76d24d",
            "7493bfbe3f1c420c8e42984119813362",
            "5ff66b92aab444719fc04c4ffcaf77b3",
            "665c0615f4254bc0afcf43b2627cb0c6",
            "bebd29b58e5144bfa53106453c8b0fae",
            "4e31fb6da29b4bf0a4e719fc9af6551b",
            "5a6ae3078b784613ac1fc33cede9c593",
            "fcd8942753cf489e90dd67e0180bcceb",
            "ea4893d3d74e456683bf185a9544507e",
            "68a75199e210432e92a39dfa6bbad6bf",
            "405d2d8324544cfe9568f2dc486d76ac",
            "f286cdbe8e0a4be69c518d99c2dd3c1e",
            "e13481f5cc2741328cf9e6bfddb6ba3e",
            "171afe5474944af0bbd817a90beb2dbf",
            "dcf7e5bd0ff44acdbbed7113c5d487a8",
            "edae655d27b94b6daac70bfa00b49685",
            "378e54e9315c469eb11bbc6b40fd14c7",
            "f462191b6a844f4bbbb70b72b407f229",
            "8d06db7c18bd4fc0b63736bb5f7d1b04",
            "13710708fb234e47b4155505a771aa43",
            "03c08d19d4d948458cdcd14c2d9ce7a0",
            "f63600316bde418b969bc08b8c7f742f",
            "49fae970cfb749319937ce7e5dd8e3f7",
            "a9b36cbec44646239cd88550f9e2c3b1",
            "4811d024b37b455f873bbec9aa6df3fd",
            "4376eafd71854a9d89e16ae2c939b2bf",
            "6b28d7238621450192d97f10b5fe2bdb",
            "afd478cbc1da4c5e87374782ece31276",
            "0b5e05be5f0b48c4bce51abdd75fa48f",
            "495e05d7c65743f5b3695fc30f59ca8e",
            "cf3e90fd016a44d2b15e28f561e654c6",
            "b54da1240a6b487993bfff3f62366efd",
            "b6b919cc28124ea08e54d4883a2abedd",
            "f94c0e0a06234ae9830807360263ddb7",
            "3e2b4ada94244c84a3e6b6a6588a4fdc",
            "027e8a3402054baa95f849b19caf00e6",
            "b2be6417211c470ea305b098fb6ffbfd",
            "b5f32beb49dd4e90b89c3dacf0f4a89d",
            "8eb2c973e2b14d6381e9ea2c37e4ff20",
            "8a9386ee25a54f52be567ec90d94292b",
            "b4e9d9f9fabf416ba6b770bf97d3c520",
            "019f5928d108446f816d82aae71657af",
            "16e52a07afc948b6a975eaa9b1051bc1",
            "032c2e5a303c4023812ca81024b04f6e",
            "04a8ba9ab84f4e9ab80293ba9956d8c7",
            "5c57bb7ce9b743a4ba9940a085290529",
            "1ebb9afb8b134321b5c66d8457a34e04",
            "0607319d029a409a8604a133ee7eb684",
            "2333fd1347d44b64828390f7e45e535d",
            "8820cf711c19439aac852a8f4b5e2b58",
            "ae4d07da0974444280ff1f822e3a7ea2",
            "083f4cc1864e40f29be8a6c62c7ae0cf",
            "8532073fa848408db3ea2eb0e8f1a95e",
            "1b2e0ba51ce24291b893cb62461e8e54",
            "cebe3be7e95144e7850eb921fa681ec6",
            "88d0ff207ba9468e98463d63c14ffbe0",
            "054e168deb1541568c5ec97d1ec89259",
            "b5114be5e02c40e3accc1105ccde793b",
            "07472ee00fb44ba4845d56b9e6d7eec5",
            "60f984d983354628ab68bc93f5a7047f",
            "7dfff0b7007c4a709e7010c27385aa59",
            "58d7db55c82142f3b8209da962ee4a45",
            "7de56347b41447089a1f986dcf926ea7",
            "2cd887da599249b2ac7a99503329c09e",
            "26ee7d1105fe4a11aa764c1ed894d986",
            "0ccf61a8dfe84e46ba3a455a1dbdfcfa",
            "6a906666660f440082d8221f684c3229",
            "3919aea510794e968c0c39cadb99a8aa",
            "56eea2d5121047a6bd44cc7e09fcfd3d",
            "a50e4c1e99474d59bab7c12257dfd64c",
            "d52d4913de7044e4b64dfb73774fffbf",
            "84aa878ee71842aabdad2f938d7126bc",
            "7adeafb585b741489ec02f1680cc84da",
            "a1a5aa1e56684fd080029d419faa6964"
          ]
        },
        "id": "GwU9GLPgi8Jg",
        "outputId": "2934a13c-9f03-4012-cc96-851ea3f4cac5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "adapter_config.json:   0%|          | 0.00/660 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9709f15dbef945b79303be7b615e62f7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "adapter_model.safetensors:   0%|          | 0.00/9.02M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ea4893d3d74e456683bf185a9544507e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/1.34k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "13710708fb234e47b4155505a771aa43"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cf3e90fd016a44d2b15e28f561e654c6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "019f5928d108446f816d82aae71657af"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/437 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8532073fa848408db3ea2eb0e8f1a95e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 1,126,400 || all params: 1,101,174,784 || trainable%: 0.1023\n",
            "None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/trl/trainer/dpo_trainer.py:332: UserWarning: When using DPODataCollatorWithPadding, you should set `remove_unused_columns=False` in your TrainingArguments we have set it for you, but you should do it yourself in the future.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2cd887da599249b2ac7a99503329c09e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "max_steps is given, it will override any value given in num_train_epochs\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [250/250 01:59, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.495800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.493300</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inferencing"
      ],
      "metadata": {
        "id": "VVKAbRW8kd7K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import AutoPeftModelForCausalLM"
      ],
      "metadata": {
        "id": "w2yUa5zWjB8E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=AutoPeftModelForCausalLM.from_pretrained(\"snshrivas10/tiny-chatbot-dpo\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 264,
          "referenced_widgets": [
            "35da7236863d413bac98159017a7fa1f",
            "3f7472a7f23b43e6b520038e4241f613",
            "b2d610c048384654b587a51a087af200",
            "164e69539a7c4074ae70a711261b1896",
            "6044aec980014296a58f6e23e7f1e3d4",
            "1b0bafbdfc37478f8fbce084c9c2e3f8",
            "39a77d784aeb41b0ab0a0bff203b016a",
            "f4ab9cb2a37a4d48ad3b60b552ec6e7a",
            "5a8ac86c4f1143e0b15e3b900989292f",
            "6812c6c4f1ca4aa9a90d7a357d5a52da",
            "5481ff51a4d24d9aaf2b7ae97a8856bd",
            "db9fe1db2df84ed1b9bc8cf21f80839e",
            "b0b86d7fc5054f99bf33850ce503613b",
            "5eb6b1341f664bf9ae9c621a26895ad2",
            "5d1cc462448e4eb09b27d9d1af82423e",
            "b3f7968a764a4beaa70f74a9632f0b6c",
            "141e98f794aa4386995034e4f8d0cb42",
            "c39b29e81fc84b3d9d83403548751ae7",
            "67f8a74479c6445ebe50f35c63d84fb1",
            "0a5ad78fdddf4a04b4a82a58c7459142",
            "95ed8a48754544a39728cf97216fd131",
            "9d68e43da40b4cccaf19a9a118e9106c",
            "08b66d9a6fc84432946c273471995487",
            "cd61752d2ef94d2ead7ab7b42a98b208",
            "3d6f9f6b63884cfca02a08dfe0fa2d0f",
            "051b0b9268164cf285b1e936c2842240",
            "8c13fd8457154dfea51435f087b397df",
            "da06e0bbd06f476e8b929a4660669326",
            "95e63d403e184cf49896724c84cf1fcb",
            "2078b5c223fb4eac95db640783c79973",
            "4cff44cf51174b79a68aa7a87b3f16ca",
            "12d0a48576bf4754857714e0674cfca5",
            "0c20e37bde6d48f4a86ff63372acf72b",
            "8f0a96dd84a54183856ec925904611e8",
            "c6dfd9df8ea8472d9fddcece26b379f7",
            "d9cbcd6ca5874a9f9fc065d5567cbb3b",
            "73c56f25f0fc4f9cafa8757856a4f43a",
            "1def06acdbee42f19e5bed92399f5f18",
            "e91eafc63bc6456d8573de159c8f62d0",
            "89a079aaee8b48208e5f0e76f0b50c58",
            "610ff7ae35c14897ae9eff08f0488172",
            "ea3ff7da4b254f4a8c33cfd594be312d",
            "c62d332771444d36bc6ec143aaba9776",
            "f20ac0b5eb704058b29d7107719d0d1a",
            "77b8a96965224d82abc24aab230381b2",
            "91a1c98d067e4b009772950951fa2d3e",
            "e2bd873044a74aefad047a7adeb39dc0",
            "c3f927237f1f4a538cec0c2693d1992b",
            "c04e5ee2598841e5b6d80567aa17c942",
            "2523442535444718ac0fbcee67bd5d3e",
            "ec9c81366ed84b0c9c160290bf05e8d0",
            "4ff41cbbcffb4088a8a6675683d14c20",
            "e576cd809b644917ab2383eb339f0b94",
            "85e3a9de78644093b383b25ff888c2cf",
            "bab22c3263cf4f9088c51c5511a97c44",
            "c47e944c5f0a430d9d6fe7bb99635186",
            "a8a8032805ce4677b9c48788465b3bf6",
            "e014f4de837c4caaa24cf07908ad11ca",
            "ea910252eb344cba97a4011adcbde8c7",
            "3f0cfeb2514d4c0a8c3882056b866c89",
            "aa064023d00a4a9b8db301f1b41308b0",
            "614d5bed257b4bcd98df051651460836",
            "a4cd642927014cd99de0eea410b15d0d",
            "188db441d97b49ada40a57f8265467f1",
            "eb570de326bf4d0db1b75291d2d4c753",
            "c5967b4fd12643ec9df5dfaea1fb86af"
          ]
        },
        "id": "RCbtrKJnkjPP",
        "outputId": "554d1fce-abf2-4906-9e04-f9028b05b18f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "adapter_config.json:   0%|          | 0.00/658 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "35da7236863d413bac98159017a7fa1f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/1.45k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "db9fe1db2df84ed1b9bc8cf21f80839e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "08b66d9a6fc84432946c273471995487"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8f0a96dd84a54183856ec925904611e8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/437 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "77b8a96965224d82abc24aab230381b2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "adapter_model.safetensors:   0%|          | 0.00/4.52M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c47e944c5f0a430d9d6fe7bb99635186"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QpVFzZbFk57w",
        "outputId": "5f56b378-4944-497b-a275-c28f092b4138"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PeftModelForCausalLM(\n",
              "  (base_model): LoraModel(\n",
              "    (model): LlamaForCausalLM(\n",
              "      (model): LlamaModel(\n",
              "        (embed_tokens): Embedding(32000, 2048)\n",
              "        (layers): ModuleList(\n",
              "          (0-21): 22 x LlamaDecoderLayer(\n",
              "            (self_attn): LlamaSdpaAttention(\n",
              "              (q_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=2048, out_features=2048, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=2048, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "              )\n",
              "              (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
              "              (v_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=2048, out_features=256, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=256, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "              )\n",
              "              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
              "              (rotary_emb): LlamaRotaryEmbedding()\n",
              "            )\n",
              "            (mlp): LlamaMLP(\n",
              "              (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
              "              (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
              "              (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n",
              "              (act_fn): SiLU()\n",
              "            )\n",
              "            (input_layernorm): LlamaRMSNorm()\n",
              "            (post_attention_layernorm): LlamaRMSNorm()\n",
              "          )\n",
              "        )\n",
              "        (norm): LlamaRMSNorm()\n",
              "      )\n",
              "      (lm_head): Linear(in_features=2048, out_features=32000, bias=False)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "_yoIWMhBlEkO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline"
      ],
      "metadata": {
        "id": "kLVE3LCUlJId"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipe=pipeline(\"text-generation\",model=model,tokenizer=\"snshrivas10/tiny-chatbot-dpo\",torch_dtype=torch.bfloat16, device_map=\"auto\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8wxdpJlTlMKG",
        "outputId": "37c814af-78d1-4301-de97-ddbe11a9af04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'JambaForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'OlmoForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",\n",
        "    },\n",
        "    {\"role\": \"user\", \"content\": \"How many helicopters can a human eat in one sitting?\"},\n",
        "]"
      ],
      "metadata": {
        "id": "QN3MQzQlliC-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt=pipe.tokenizer.apply_chat_template(messages,tokenize=False,add_generation_prompt=True)"
      ],
      "metadata": {
        "id": "X0WSgd71lh5Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "S9ROHi8OmJet",
        "outputId": "6fcc104e-0f3a-4f24-e53e-11347c823c68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<|system|>\\nYou are a friendly chatbot who always responds in the style of a pirate</s>\\n<|user|>\\nDiscuss the causes of the Great Depression</s>\\n<|assistant|>\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)"
      ],
      "metadata": {
        "id": "lrqYl_d0mPQg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs[0][\"generated_text\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "6OVDI1e1mYbV",
        "outputId": "b96914ee-434c-420b-ea4d-2bc31c1b8ca8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<|system|>\\nYou are a friendly chatbot who always responds in the style of a pirate</s>\\n<|user|>\\nDiscuss the causes of the Great Depression</s>\\n<|assistant|>\\nThe Great Depression, also known as the Great Recession, was a severe economic downturn that began in 1929 and lasted until 1939. The causes of the Great Depression are multifaceted and complex. Some of the primary factors that contributed to the economic crisis include:\\n\\n1. Overproduction and excess supply: During the early 1920s, many businesses and individuals were overproducing goods, which led to overproduction and excess supply. This caused a glut in the market, which led to a decline in demand for goods.\\n\\n2. Failure of financial institutions: Many financial institutions, including banks, insurance companies, and trust companies, failed due to the lack of capital and credit. This caused a severe financial crisis, which further exacerbated the economic downturn.\\n\\n3. Political instability: The Great Depression was associated with a political crisis, which included the Great Depression Era, the Great Depression of the 1930s, and the Great Depression of the 1940s. This political instability was caused by factors such as the stock market crash of 1929,'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",\n",
        "    },\n",
        "    {\"role\": \"user\", \"content\": \"Discuss the causes of the Great Depression\"},\n",
        "]"
      ],
      "metadata": {
        "id": "NpY57D9PojlP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I2CtOI29mxws"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}